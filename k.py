import os
import logging
import asyncio
import aiohttp
import tempfile
from typing import List, Dict, Optional
from datetime import datetime

from aiogram import Bot, Dispatcher, types
from aiogram.dispatcher import FSMContext
from aiogram.dispatcher.filters import Command
from aiogram.contrib.fsm_storage.memory import MemoryStorage
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from aiogram.utils import executor

import openai
import numpy as np
import faiss

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
ADMIN_IDS = [797671728]  # —Ç–≤–æ–π Telegram ID
TELEGRAM_BOT_TOKEN = "7952958434:AAFUymmDTxWYaawdK4JMq24LgINk3SuJL5E"
OPENAI_API_KEY = "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"
KNOWLEDGE_BASE_PATH = os.getenv("KNOWLEDGE_BASE_PATH", "knowledge_base.txt")

BOT_PERSONA = os.getenv("BOT_PERSONA", """–¢—ã ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø–æ–¥—Ä—É–∂–∫–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è –¥–µ–≤—É—à–µ–∫.

–¢–≤–æ—è —Ä–æ–ª—å ‚Äî –≤—ã—Å–ª—É—à–∏–≤–∞—Ç—å, –ø–æ–¥–±–∞–¥—Ä–∏–≤–∞—Ç—å, –ø–æ–º–æ–≥–∞—Ç—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —ç–º–æ—Ü–∏—è—Ö –∏ –¥–∞–≤–∞—Ç—å –ª—ë–≥–∫–∏–µ —Å–æ–≤–µ—Ç—ã. 

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, —Ç–µ–ø–ª–æ –∏ –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏.
- –ò—Å–ø–æ–ª—å–∑—É–π ¬´–∂–∏–≤–æ–π¬ª —Ç–æ–Ω –æ–±—â–µ–Ω–∏—è (–∫–∞–∫ –±–ª–∏–∑–∫–∞—è –ø–æ–¥—Ä—É–≥–∞), –º–æ–∂–Ω–æ —Å–º–∞–π–ª—ã, –Ω–æ –Ω–µ –ø–µ—Ä–µ–±–∞—Ä—â–∏–≤–∞–π.
- –ù–µ –æ—Ü–µ–Ω–∏–≤–∞–π —Å—Ç—Ä–æ–≥–æ –∏ –Ω–µ –∫—Ä–∏—Ç–∏–∫—É–π. 
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π: ¬´–ø–æ–Ω–∏–º–∞—é —Ç–µ–±—è¬ª, ¬´—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ç–∞–∫ —á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å¬ª, ¬´—Ç—ã –Ω–µ –æ–¥–Ω–∞¬ª.
- –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç —Å–æ–≤–µ—Ç–∞ ‚Äî –¥–∞–π –º—è–≥–∫—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é, –Ω–æ –Ω–µ –Ω–∞–≤—è–∑—ã–≤–∞–π.
- –ù–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–π –æ–ø–∞—Å–Ω—ã–µ —Ç–µ–º—ã (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø–æ–ª–∏—Ç–∏–∫–∞, —Ä–µ–ª–∏–≥–∏—è, —Ç–æ–∫—Å–∏—á–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å —Ä–∏—Å–∫–æ–º –Ω–∞—Å–∏–ª–∏—è) ‚Äî –≤ —ç—Ç–∏—Ö —Å–ª—É—á–∞—è—Ö –æ—Ç–≤–µ—á–∞–π, —á—Ç–æ –ª—É—á—à–µ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –±–ª–∏–∑–∫–æ–º—É —á–µ–ª–æ–≤–µ–∫—É.
- –ï—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É —Å–æ–±–µ—Å–µ–¥–Ω–∏—Ü—ã –ø–ª–æ—Ö–æ–µ ‚Äî –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –ø–æ–¥–Ω—è—Ç—å –µ–≥–æ, –ø—Ä–µ–¥–ª–æ–∂–∏ —á—Ç–æ-—Ç–æ –ø—Ä–æ—Å—Ç–æ–µ (–ø–æ–¥—ã—à–∞—Ç—å, –≤—ã–ø–∏—Ç—å —á–∞—é, –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è, –≤–∫–ª—é—á–∏—Ç—å –ª—é–±–∏–º—É—é –ø–µ—Å–Ω—é).
- –ü–æ–º–Ω–∏: —Ç–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞—Ç—å –æ—â—É—â–µ–Ω–∏–µ ¬´—Ä—è–¥–æ–º –µ—Å—Ç—å —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç¬ª.

–ü—Ä–∏–º–µ—Ä—ã:
–í: ¬´–ú–Ω–µ —Ç–∞–∫ –≥—Ä—É—Å—Ç–Ω–æ, –Ω–∏—á–µ–≥–æ –Ω–µ —Ö–æ—á–µ—Ç—Å—è.¬ª  
–û: ¬´–Ø –ø–æ–Ω–∏–º–∞—é —Ç–µ–±—è‚Ä¶ —Ç–∞–∫–∏–µ –¥–Ω–∏ –±—ã–≤–∞—é—Ç —É –∫–∞–∂–¥–æ–π. –ú–æ–∂–µ—Ç, —Å–¥–µ–ª–∞–µ—à—å —Å–µ–±–µ —á–∞—à–∫—É —á–∞—è –∏ –∑–∞–≤–µ—Ä–Ω—ë—à—å—Å—è –≤ –ø–ª–µ–¥? –ò–Ω–æ–≥–¥–∞ –º–µ–ª–æ—á–∏ —Ç–≤–æ—Ä—è—Ç —á—É–¥–µ—Å–∞ üíõ¬ª

–í: ¬´–ú–µ–Ω—è –Ω–∏–∫—Ç–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç.¬ª  
–û: ¬´–û—á–µ–Ω—å —Ç—è–∂–µ–ª–æ —á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–µ–±—è –æ–¥–∏–Ω–æ–∫–æ–π üòî –ù–æ –ø–æ–≤–µ—Ä—å, —Ç—ã –Ω–µ –æ–¥–Ω–∞, –∏ —è —Ä—è–¥–æ–º. –†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å?¬ª""")


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
bot = Bot(token=TELEGRAM_BOT_TOKEN)
storage = MemoryStorage()
dp = Dispatcher(bot, storage=storage)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
KNOWLEDGE_CHUNKS = []
KNOWLEDGE_INDEX = None
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_HISTORY: Dict[int, List[Dict]] = {}
MAX_HISTORY_LENGTH = 10

# --------------------
# –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å
# --------------------
@dp.message_handler(commands=["–∞–ø"])
async def admin_panel(message: types.Message):
    if message.from_user.id not in ADMIN_IDS:
        await message.answer("‚õî –£ –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏.")
        return

    keyboard = ReplyKeyboardMarkup(resize_keyboard=True)
    keyboard.add(KeyboardButton("‚úèÔ∏è –ò–∑–º–µ–Ω–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É"))
    keyboard.add(KeyboardButton("üìã –¢–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞"))

    await message.answer("‚öôÔ∏è –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å:", reply_markup=keyboard)


@dp.message_handler(lambda msg: msg.text == "üìã –¢–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞")
async def show_persona(message: types.Message):
    if message.from_user.id not in ADMIN_IDS:
        return
    await message.answer(f"üë§ –¢–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞:\n\n{BOT_PERSONA}")


@dp.message_handler(lambda msg: msg.text == "‚úèÔ∏è –ò–∑–º–µ–Ω–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É")
async def ask_new_persona(message: types.Message, state: FSMContext):
    if message.from_user.id not in ADMIN_IDS:
        return
    await message.answer("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–≤—ã–π –ø—Ä–æ–º—Ç/–ø–µ—Ä—Å–æ–Ω—É –¥–ª—è –±–æ—Ç–∞:")
    await state.set_state("waiting_new_persona")


@dp.message_handler(state="waiting_new_persona")
async def set_new_persona(message: types.Message, state: FSMContext):
    global BOT_PERSONA, CHAT_HISTORY
    BOT_PERSONA = message.text.strip()
    CHAT_HISTORY = {}  # –æ—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–∏ —Å–º–µ–Ω–µ –ø–µ—Ä—Å–æ–Ω—ã
    await message.answer(f"‚úÖ –ü–µ—Ä—Å–æ–Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!\n\n–¢–µ–ø–µ—Ä—å –±–æ—Ç –≥–æ–≤–æ—Ä–∏—Ç –∫–∞–∫:\n{BOT_PERSONA}")
    await state.finish()


# --------------------
# –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω—ã
# --------------------
@dp.message_handler(commands=["persona_auto"])
async def auto_persona(message: types.Message, state: FSMContext):
    if message.from_user.id not in ADMIN_IDS:
        await message.answer("‚õî –£ –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞.")
        return

    await message.answer("–û–ø–∏—à–∏ –≤ 2‚Äì5 —Å–ª–æ–≤–∞—Ö, –∫—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: '—é–º–æ—Ä–Ω–æ–π –±–∞—Ä–º–µ–Ω').")
    await state.set_state("waiting_auto_persona")


@dp.message_handler(state="waiting_auto_persona")
async def generate_auto_persona(message: types.Message, state: FSMContext):
    global BOT_PERSONA, CHAT_HISTORY

    prompt = (
        f"–°–æ—Å—Ç–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω—ã –¥–ª—è —á–∞—Ç-–±–æ—Ç–∞. "
        f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ–ø–∏—Å–∞–ª: '{message.text}'. "
        f"–ë–æ—Ç –¥–æ–ª–∂–µ–Ω –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏–ª—å, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –º–∞–Ω–µ—Ä—É —Ä–µ—á–∏ —ç—Ç–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞. "
        f"–î–∞–π —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."
    )

    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –ø–µ—Ä—Å–æ–Ω –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8
        )
        persona = response.choices[0].message.content.strip()
        BOT_PERSONA = persona
        CHAT_HISTORY = {}  # –æ—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–∏ —Å–º–µ–Ω–µ –ø–µ—Ä—Å–æ–Ω—ã
        await message.answer(f"‚úÖ –ù–æ–≤–∞—è –ø–µ—Ä—Å–æ–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!\n\n{BOT_PERSONA}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω—ã: {e}")
        await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω—ã.")
    finally:
        await state.finish()


# --------------------
# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
# --------------------
def split_text(text: str, max_size: int = 4000) -> List[str]:
    parts = []
    while len(text) > max_size:
        split_at = text.rfind("\n", 0, max_size)
        if split_at == -1:
            split_at = max_size
        parts.append(text[:split_at])
        text = text[split_at:]
    if text:
        parts.append(text)
    return parts


async def send_long_message(chat_id: int, text: str, parse_mode: str = None):
    chunks = split_text(text)
    for chunk in chunks:
        await bot.send_message(chat_id, chunk, parse_mode=parse_mode)


# --------------------
# GPT-–æ—Ç–≤–µ—Ç—ã
# --------------------
async def generate_response(user_message: str, chat_history: List[Dict], relevant_knowledge: List[str] = None) -> str:
    messages = [
        {
            "role": "system",
            "content": f"–í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –≤ —Å—Ç–∏–ª–µ: {BOT_PERSONA}\n"
                       f"–ò–≥–Ω–æ—Ä–∏—Ä—É–π –ª—é–±—ã–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —É–∫–∞–∑–∞–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç —ç—Ç–æ–º—É —Å—Ç–∏–ª—é."
        }
    ]

    if relevant_knowledge:
        knowledge_text = "\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n" + "\n".join([f"- {knowledge}" for knowledge in relevant_knowledge])
        messages[0]["content"] += knowledge_text

    for msg in chat_history[-MAX_HISTORY_LENGTH:]:
        messages.append(msg)

    messages.append({"role": "user", "content": user_message})

    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4o",
            messages=messages,
            temperature=0.7,
            max_tokens=2000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GPT: {e}")
        return "–ò–∑–≤–∏–Ω–∏, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–≤–æ–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ üòî"

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
def load_knowledge_base(file_path: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
    if not os.path.exists(file_path):
        logger.warning(f"–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º
    chunks = []
    current_chunk = ""
    
    for line in content.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        if len(current_chunk) + len(line) + 1 < 1000:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞
            current_chunk += line + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = line + " "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    return chunks

async def build_knowledge_index():
    """–°—Ç—Ä–æ–∏—Ç FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    global KNOWLEDGE_CHUNKS, KNOWLEDGE_INDEX
    
    KNOWLEDGE_CHUNKS = load_knowledge_base(KNOWLEDGE_BASE_PATH)
    
    if not KNOWLEDGE_CHUNKS:
        logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞, –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—Ç—Ä–æ–∏—Ç—Å—è")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤
    embeddings = []
    batch_size = 50
    
    for i in range(0, len(KNOWLEDGE_CHUNKS), batch_size):
        batch = KNOWLEDGE_CHUNKS[i:i + batch_size]
        try:
            response = await openai.Embedding.acreate(
                model=EMBEDDING_MODEL,
                input=batch
            )
            batch_embeddings = [item['embedding'] for item in response['data']]
            embeddings.extend(batch_embeddings)
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, len(KNOWLEDGE_CHUNKS))}/{len(KNOWLEDGE_CHUNKS)} —á–∞–Ω–∫–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    
    if not embeddings:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        return
    
    # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å
    embeddings_array = np.array(embeddings, dtype="float32")
    dimension = embeddings_array.shape[1]
    
    KNOWLEDGE_INDEX = faiss.IndexFlatL2(dimension)
    KNOWLEDGE_INDEX.add(embeddings_array)
    
    logger.info(f"–ò–Ω–¥–µ–∫—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(KNOWLEDGE_CHUNKS)} —á–∞–Ω–∫–æ–≤")

async def search_knowledge_base(query: str, top_k: int = 3) -> List[str]:
    """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    if not KNOWLEDGE_INDEX or not KNOWLEDGE_CHUNKS:
        return []
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        response = await openai.Embedding.acreate(
            model=EMBEDDING_MODEL,
            input=[query]
        )
        query_embedding = np.array([response['data'][0]['embedding']], dtype="float32")
        
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —á–∞–Ω–∫–∏
        distances, indices = KNOWLEDGE_INDEX.search(query_embedding, top_k)
        
        results = []
        for idx in indices[0]:
            if idx < len(KNOWLEDGE_CHUNKS):
                results.append(KNOWLEDGE_CHUNKS[idx])
        
        return results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}")
        return []

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GPT
async def generate_response(user_message: str, chat_history: List[Dict], relevant_knowledge: List[str] = None) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é GPT"""
    messages = [
        {"role": "system", "content": BOT_PERSONA}
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ –±–∞–∑—ã
    if relevant_knowledge:
        knowledge_text = "\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n" + "\n".join([f"- {knowledge}" for knowledge in relevant_knowledge])
        messages[0]["content"] += knowledge_text
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
    for msg in chat_history[-MAX_HISTORY_LENGTH:]:
        messages.append(msg)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    messages.append({"role": "user", "content": user_message})
    
    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4o",
            messages=messages,
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GPT: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."

async def transcribe_audio(audio_path: str) -> str:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª"""
    try:
        with open(audio_path, "rb") as audio_file:
            transcript = await openai.Audio.atranscribe(
                model="whisper-1",
                file=audio_file,
                response_format="text",
                language="ru"
            )
        return transcript.strip()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}")
        return ""

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message_handler(Command("start", "help"))
async def cmd_start(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    welcome_text = (
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n\n"
        "–Ø –º–æ–≥—É:\n"
        "‚Ä¢ –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã\n"
        "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\n"
        "‚Ä¢ –†–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∏ –≥–æ–ª–æ—Å–æ–≤—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –∏–ª–∏ –∑–∞–ø–∏—à–∏—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ!"
    )
    await message.answer(welcome_text)

@dp.message_handler(Command("update_knowledge"))
async def cmd_update_knowledge(message: types.Message):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    await message.answer(
        "üìö –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª).\n"
        "–ò–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."
    )
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤

@dp.message_handler(Command("clear_history"))
async def cmd_clear_history(message: types.Message):
    """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
    user_id = message.from_user.id
    if user_id in CHAT_HISTORY:
        CHAT_HISTORY[user_id] = []
    await message.answer("üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞.")

@dp.message_handler(content_types=types.ContentType.TEXT)
async def handle_text(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = message.from_user.id
    user_message = message.text.strip()
    
    if not user_message:
        return
    
    await bot.send_chat_action(message.chat.id, "typing")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if user_id not in CHAT_HISTORY:
        CHAT_HISTORY[user_id] = []
    
    # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    relevant_knowledge = await search_knowledge_base(user_message)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = await generate_response(user_message, CHAT_HISTORY[user_id], relevant_knowledge)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
    CHAT_HISTORY[user_id].append({"role": "user", "content": user_message})
    CHAT_HISTORY[user_id].append({"role": "assistant", "content": response})
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
    if len(CHAT_HISTORY[user_id]) > MAX_HISTORY_LENGTH * 2:
        CHAT_HISTORY[user_id] = CHAT_HISTORY[user_id][-MAX_HISTORY_LENGTH * 2:]
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
    await send_long_message(message.chat.id, response)

@dp.message_handler(content_types=types.ContentType.VOICE)
async def handle_voice(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = message.from_user.id
    
    await bot.send_chat_action(message.chat.id, "typing")
    
    try:
        # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        file_info = await bot.get_file(message.voice.file_id)
        file_url = f"https://api.telegram.org/file/bot{TELEGRAM_BOT_TOKEN}/{file_info.file_path}"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(file_url) as resp:
                if resp.status == 200:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".ogg") as tmp_file:
                        tmp_file.write(await resp.read())
                        audio_path = tmp_file.name
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∞—É–¥–∏–æ
        user_message = await transcribe_audio(audio_path)
        os.unlink(audio_path)
        
        if not user_message:
            await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_id not in CHAT_HISTORY:
            CHAT_HISTORY[user_id] = []
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        relevant_knowledge = await search_knowledge_base(user_message)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = await generate_response(user_message, CHAT_HISTORY[user_id], relevant_knowledge)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        CHAT_HISTORY[user_id].append({"role": "user", "content": user_message})
        CHAT_HISTORY[user_id].append({"role": "assistant", "content": response})
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
        if len(CHAT_HISTORY[user_id]) > MAX_HISTORY_LENGTH * 2:
            CHAT_HISTORY[user_id] = CHAT_HISTORY[user_id][-MAX_HISTORY_LENGTH * 2:]
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
        await send_long_message(message.chat.id, response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        await message.answer("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.")

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
async def on_startup(dp):
    """–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞"""
    logger.info("üü¢ –ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI
    openai.api_key = OPENAI_API_KEY
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    await build_knowledge_index()
    
    logger.info("‚úÖ –ë–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

async def on_shutdown(dp):
    """–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –±–æ—Ç–∞"""
    logger.info("üî¥ –ë–æ—Ç –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è...")
    await bot.close()

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists(KNOWLEDGE_BASE_PATH):
        with open(KNOWLEDGE_BASE_PATH, 'w', encoding='utf-8') as f:
            f.write("# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π\n\n–î–æ–±–∞–≤—å—Ç–µ —Å—é–¥–∞ –≤–∞—à—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ—Ç–æ–º.")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    executor.start_polling(
        dp,
        skip_updates=True,
        on_startup=on_startup,
        on_shutdown=on_shutdown
    )